---
title: "MainCleaner0924"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

# Libraries

```{r}
library(googlesheets4)
library(conflicted)
library(tidyverse)
conflicts_prefer(dplyr::filter)
conflicts_prefer(dplyr::select)
library(base64enc)
```

CHECK WHICH OF THESE LIBRARIES ACTUALLY ARE NEEDED

```{r}
library(data.table)
library(zlib)
library(digest)
library(openssl)
```

# Data

## Google

```{r}
ss <- "https://docs.google.com/spreadsheets/d/1xzpre5Ej_7OEGRU4EA7KZuMQnSz5YCyTx5Sdbml6bQE/edit#gid=0"

# Old scrapes
## Swedish scrape
se_Old <- read_sheet(ss, sheet = "se_scrape")

## Norwegian scrape
no_Old <- read_sheet(ss, sheet = "no_scrape")

## Danish scrape
dk_Old <- read_sheet(ss, sheet = "dk_scrape")

# Company name chart
comp_names <- read_sheet(ss, sheet = "company_names")

# Location name chart with extra info
locations <- read_sheet(ss, sheet = "locations_coord")
```

## Latest scrape

For this section to work must the scrapes have been downloaded and put into a local folder which the main directory can access. It is assumed that the Swedish and Norwegian data is in a csv file while the Danish is in a rds file

```{r}
# Swedish scrape
ams_Latest_scrape <- read.csv("ams.csv", sep=";") %>% filter(s_terms.i.=="vätgas*")

# Norwegian scrape
finn_Latest_scrape <- read.csv("finn_no_h2.csv")

#Danish scrape
dk_Latest_scrape <- as.data.frame(readRDS("dk_h2.rds")) 
```

# Wrangle of scrape

## Prepping a dataframe

```{r}
new_data <- data.frame( ID = numeric(0), Title = numeric(0), Company = numeric(0), 
                        Location = numeric(0), Description = numeric(0), Scrape_date = numeric(0),
                        Hydrogen_relevance = numeric(0), translatedText = numeric(0))
```

## Refining latest scrapes

### Sweden

```{r}
#Compare what is new
new_se <- ams_Latest_scrape[!ams_Latest_scrape$links %in% se_Old$links,] 

# Add ID to our new data
ID <- nrow(se_Old)
IDcol <- c()
for (i in seq_len(nrow(new_se))) {
  ID <- ID + 1
  IDcol <- append(IDcol,10000+ID)
}
new_se <- cbind(IDcol,new_se)

# Choose what to put into clean sheet
new_for_clean_se <- new_se %>% select(IDcol,job_role,company,location,description,scrape_date)
colnames(new_for_clean_se) <- c("ID","Title", "Company","Location","Description","Scrape_date")

# Add to total of new data
new_for_clean_se[c("Hydrogen_relevance", "translatedText")] <- NA
new_data <- rbind(new_data,new_for_clean_se)
```

### Norway

```{r}
#Compare what is new
new_no <- finn_Latest_scrape[!finn_Latest_scrape$url %in% no_Old$url,] 

# Add ID to our new data
ID <- nrow(no_Old)
IDcol <- c()
for (i in seq_len(nrow(new_no))) {
  ID <- ID + 1
  IDcol <- append(IDcol,20000+ID)
}
new_no <- cbind(IDcol,new_no)

# Choose what to put into clean sheet
new_for_clean_no <- new_no %>% select(IDcol,stillingstittel,arbetsgivare,sted,add_text,id)

# We want the date for the scrape to be in year-month-day
for (i in seq_len(nrow(new_for_clean_no))) {
  new_for_clean_no$id[i] <- substr(new_for_clean_no$id[i], start = 1, stop = 10)
}

# Change column names
colnames(new_for_clean_no) <- c("ID","Title", "Company","Location","Description","Scrape_date")

# Add to total of new data
new_for_clean_no[c("Hydrogen_relevance", "translatedText")] <- NA
new_data <- rbind(new_data,new_for_clean_no)
```

### Denmark

```{r}
#Compare what is new
new_dk <- dk_Latest_scrape[!dk_Latest_scrape$link_to_external_add %in% dk_Old$link_to_external_add,] 

# Add ID to our new data
ID <- nrow(dk_Old)
IDcol <- c()
for (i in seq_len(nrow(new_dk))) {
  ID <- ID + 1
  IDcol <- append(IDcol,30000+ID)
}
new_dk <- cbind(IDcol,new_dk)
```

We have an issue with the danish file. It is the "all_text" column which consists of cells containing more than 50k characters (which is the limit for google sheet). For this problem there are two solutions:

-   To shorten the texts by removing non-essential text

-   To compress the text into a shorter string. This string is then split into three different cells

For the sake of saving all information is both done.

```{r}
# Make a short_text column with "cleaned" text
remove_lines_with_pattern <- function(text, pattern) {
  # Pattern to match an entire line containing the specified pattern
  line_pattern <- paste0(".*", pattern, ".*\n?")
  str_replace_all(text, line_pattern, "")
}

pattern <- c("Cookiebot|DOMAINS|HTTP|www|/privacy|HTML|Thumbnails|Zoom|Cookie|cookie|ID|CONSENT|Samtykke|domains|consent|window|button|Expiry|SessionType:|/|\\{|\\}")

ddk <- new_dk %>% 
  select(IDcol, all_text) %>%
  mutate(nr_breaks = str_count(all_text, pattern = "\n")) %>%
  mutate(nr_dots = str_count(all_text, pattern = "\\.")) %>%
  mutate(all_text_chr = str_length(all_text))

ddk_clean_breaks <- ddk %>%
  mutate(clean_breaks = map_chr(all_text, ~remove_lines_with_pattern(., pattern), .progress = T)) %>%
  mutate(clean_chr = str_length(clean_breaks))

df_short <- ddk_clean_breaks %>% select(IDcol, short_text = clean_breaks) %>% unique() 

dk_short <- new_dk %>% 
  left_join(df_short) %>%
  mutate(all_text_chr = str_length(all_text),
         short_text_chr = str_length(short_text)) 


new_dk$short_text <- dk_short$short_text

#-------------------------------------------------------------------------------------------#

# Define a function to compress a string
compress_string <- function(x) {
  compressed <- memCompress(charToRaw(x), type = "gzip")
  base64encode(compressed)
}

# Apply the function to the column in dataframe
new_dk$all_text <- lapply(new_dk$all_text, compress_string)

# Some tsrings are still too long in all_text, we will split them in three
split_string <- function(x) {
  n <- nchar(x)
  c1 <- substr(x, 1, min(50000, n))
  c2 <- ifelse(n > 50000, substr(x, 50001, min(100000, n)), "")
  c3 <- ifelse(n > 100000, substr(x, 100001, n), "")
  return(c(c1, c2, c3))
}

# Apply the function to dataframe
new_dk[c("all_text1", "all_text2", "all_text3")] <- t(apply(new_dk["all_text"], 1, split_string))

# Remove all_text
new_dk <- subset(new_dk, select = -all_text )
```

```{r}
# Choose what to put into clean sheet
new_for_clean_dk <- new_dk %>% select(IDcol,title,company,location,short_text,id)

# We want the date for the scrape to be in year-month-day
for (i in seq_len(nrow(new_for_clean_dk))) {
  new_for_clean_dk$id[i] <- substr(new_for_clean_dk$id[i], start = 1, stop = 10)
}

# Change column names
colnames(new_for_clean_dk) <- c("ID","Title", "Company","Location","Description","Scrape_date")

# Before the text is added we must check if any string still is too long - if so it cannot be uploaded 
for (i in seq_len(nrow(dk_short))) {
  if (dk_short$short_text_chr[i] > 50000) {
    new_dk$short_text[i] <- "UNSHORTABLE"
    new_for_clean_dk$Description[i] <- paste("UNSHORTABLE -", new_dk$text[i], sep=" ")
  }
}

# Add to total of new data
new_for_clean_dk[c("Hydrogen_relevance", "translatedText")] <- NA
new_data <- rbind(new_data,new_for_clean_dk)
```

The function used to de-compress the compressed text is found in a separate section here.

```{r}
# How to de-compress

#decompress_string <- function(x) {
#  decompressed <- memDecompress(base64decode(x), type = "gzip")
#  rawToChar(decompressed)
#}

#dk_Latest_scrape$all_text <- lapply(dk_Latest_scrape$all_text, decompress_string)
```

## Clean & rename

### Companies

Replaces the names of the companies in "new_data" with the new names from the worksheet, if there is no match it will be named "!!!NEW_COMPANY!!!".

```{r}
data_new_names <- new_data %>% 
  mutate(Company = case_when(tolower(str_trim(Company)) %in% tolower(comp_names$Old_name) ~ comp_names$New_name[match(tolower(str_trim(Company)),tolower(comp_names$Old_name))], TRUE ~ "!!!NEW_COMPANY!!!"))
```

### Locations

```{r}
for (i in seq_len(nrow(new_data))) {
  nr_matching <- 0
  for (j in seq_len(nrow(locations))) {
    if (grepl(locations$Original_name[j], new_data$Location[i], ignore.case = TRUE)) {
      #if true
      match <- locations$New_name[j] # save matching location name
      nr_matching <- nr_matching+1 # document how many matches we get
    } 
  }
  if (nr_matching == 0) {
    # if no matches were found must the locations list be updated to include more locations
    data_new_names$Location[i] <- "!!!NEW_LOCATION!!!"
  } else if (nr_matching > 1) {
    # if more than 1 match was found will the location choice have to be made manually
    data_new_names$Location[i] <- "!!!MULTIPLE_LOCATIONS!!!"
  } else {
    # when nr of matches is not 0 nor more than 1 will the found match be assigned to the add
    data_new_names$Location[i] <- match 
  }
}
```

# Upload changes to Google sheet

Update Main-sheet

```{r}
sheet_append(ss, data_new_names, sheet = 'Main')
```

Update scrape sheets

```{r}
# Sweden
sheet_append(ss, new_se, sheet = 'se_scrape')

# Norway
sheet_append(ss, new_no, sheet = 'no_scrape')

# Denmark
sheet_append(ss, new_dk, sheet = 'dk_scrape')
```

# Missing companies

This part of the script is for when new companies has to be added to our list of companies. It is assumed that all information is stored in the Google sheet at this point.

## Find missing companies

### Data

```{r}
ss <- "https://docs.google.com/spreadsheets/d/1xzpre5Ej_7OEGRU4EA7KZuMQnSz5YCyTx5Sdbml6bQE/edit#gid=0"

# Missing companies
new_comp <- read_sheet(ss, sheet = "Main") %>% 
  filter(Company %in% c('!!!UNKNOWN_COMPANY!!!',"!!!NEW_COMPANY!!!")) %>% 
  select(ID,Company)

# Information from scrape
## Sweden
se_comp <- read_sheet(ss,sheet = "se_scrape") %>% 
  select(IDcol,company) %>% 
  rename(ID = IDcol, Company = company)

## Norway
no_comp <- read_sheet(ss,sheet = "no_scrape") %>% 
  select(IDcol,arbetsgivare) %>% 
  rename(ID = IDcol, Company = arbetsgivare)

## Denmark
dk_comp <- read_sheet(ss,sheet = "dk_scrape") %>% 
  select(IDcol,company) %>% 
  rename(ID = IDcol, Company = company)
```

### Wrangle

Collect list of company names from the scrapes.

```{r}
all_comp <- bind_rows(se_comp, no_comp, dk_comp)

new_comp_names <- all_comp %>% 
  filter(ID %in% new_comp$ID) %>% 
  select(ID, Company)
```

At this point is it easiest to add the new companies to the sheet "company_names" manually. In the variable "new_comp_names" should you find the ID of ads with missing company as well what company is given in the scrape. ID starting with 1 is a Swedish ad, 2 Norwegian and 3 a Danish ad. If the information here is not enough, then it is easiest to either read the description of the specific ad in Google sheet or visit the site of the ad, also found in Google sheet by filtering the ID.

## Add new companies

After each missing company has been added to "company_names" can this part be run in order to update the Main-sheet.

### Data

```{r}
#Main sheet
main <- read_sheet(ss, sheet = "Main")

# Chart for company names
comp_names <- read_sheet(ss, sheet = "company_names")
```

### Wrangle

```{r}
# Get ID of missing ads with missing company
new_comp <- main %>% 
  filter(Company == "!!!NEW_COMPANY!!!") %>% 
  select(ID,Company)

#Update the company names with the company_names work sheet. Companies that still cant be found will be named !!!UNKNOWN_COMPANY!!! and there can still be some companies named !!!NEW_COMPANY!!! 
to_add <- all_comp %>% 
  filter(ID %in% new_comp$ID) %>% 
  mutate(Company = case_when(tolower(str_trim(Company)) %in% tolower(comp_names$Old_name) ~ comp_names$New_name[match(tolower(str_trim(Company)),tolower(comp_names$Old_name))], TRUE ~ "!!!UNKNOWN_COMPANY!!!"))

# Update the 'ID' and 'Company' columns in the 'Main' worksheet
main <- main %>%
  mutate(ID = ifelse(ID %in% to_add$ID, to_add$ID[match(ID, to_add$ID)], ID),
         Company = ifelse(ID %in% to_add$ID, to_add$Company[match(ID, to_add$ID)], Company))
```

At this point if there are any company missing it is easier to fix this manually in the Google sheet. Check the information from the scrape —\> Make sure the different names for the company are in "company_names" —\> Update Main-sheet.

### Upload changes

```{r}
write_sheet(main, ss = ss, sheet = "Main")
```

# COMPANY DESCRIPTION FINDER ADDED HERE???

# Missing locations

This part of the script is for when new locations has to be added to our list of locations. It is assumed that all information is stored in the Google sheet at this point.

## Find missing locations

### Data

```{r}
ss <- "https://docs.google.com/spreadsheets/d/1xzpre5Ej_7OEGRU4EA7KZuMQnSz5YCyTx5Sdbml6bQE/edit#gid=0"

# Get the ads with missing locations
adds <- read_sheet(ss, sheet = "Main")  %>% 
  select(ID,Location)

# Information from scrapes - also change names of columns
## Sweden
se_loc <- read_sheet(ss, sheet = "se_scrape") %>% 
  select(IDcol,location)
colnames(se_loc) <- c("ID","Location")

## Norway
no_loc <- read_sheet(ss, sheet = "no_scrape") %>% 
  select(IDcol,sted)
colnames(no_loc) <- c("ID","Location")

## Denmark
dk_loc <- read_sheet(ss, sheet = "dk_scrape") %>% 
  select(IDcol,location)
colnames(dk_loc) <- c("ID","Location")
```

### Wrangle

```{r}
missingLocations <- adds %>% filter(Location == "!!!NEW_LOCATION!!!")

names_of_missing <- rbind(merge(se_loc, missingLocations["ID"], by = "ID"),merge(no_loc, missingLocations["ID"], by = "ID"),merge(dk_loc, missingLocations["ID"], by = "ID"))
```

Now we have a list of the ads and locations that are missing in the sheet. These locations will have to be added manually to the "locations_coord" sheet. What needs to be added to the sheet:

-   Original name used for the "city" in the scrape

-   The new common name given (usually the English name)

-   Country of the city

-   Region of the city

-   Population of region

-   Longitude and Latitude

Other columns can be filled but are not necessary.

## Add new locations

### Data

```{r}
#Main sheet
main <- read_sheet(ss, sheet = "Main")

# Chart for locations
locations <- read_sheet(ss, sheet = "locations_coord")
```

### Wrangle

```{r}
# Add the newly updated info from Google sheet
for (i in seq_len(nrow(names_of_missing))) {
  nr_matching <- 0
  for (j in seq_len(nrow(locations))) {
    if (grepl(locations$Name[j], names_of_missing$Location[i], ignore.case = TRUE)) {
      #if true
      match <- locations$Name2[j] # save matching location name
      nr_matching <- nr_matching+1 # document how many matches we get
    } 
  }
  if (nr_matching == 0) {
    # if no matches were found add this point it means that no location has been given
    missingLocations$Location[i] <- "!!!UNKNOWN_LOCATION!!!"
  } else if (nr_matching > 1) {
    # if more than 1 match was found will the location choice have to be made manually
    missingLocations$Location[i] <- "!!!MULTIPLE_LOCATIONS!!!"
  } else {
    # when nr of matches is not 0 nor more than 1 will the found match be assigned to the add
    missingLocations$Location[i] <- match 
  }
}

adds$Location[match(missingLocations$ID, adds$ID)] <- missingLocations$Location

main <- main %>%
  mutate(ID = ifelse(ID %in% adds$ID, adds$ID[match(ID, adds$ID)], ID),
         Location = ifelse(ID %in% adds$ID, adds$Location[match(ID, adds$ID)], Location))
```

At this point there may be some unknown locations. These will have to be found in the descriptions of the ads. This is not done in the script but manually in Google sheet. Also some ads may have multiple location names in their scrape, this will also have to be resolved manually.

### Upload changes

```{r}
write_sheet(main, ss = sheet, sheet = "Main")
```
